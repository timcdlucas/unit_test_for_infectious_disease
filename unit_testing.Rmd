---
title: |
  | Responsible modelling: Unit testing for infectious disease epidemiology. +1
author:
  - name: Tim CD Lucas
    email: timcdlucas@gmail.com
    affiliation: Big Data Institute
    corresponding: timcdlucas@gmail.com
  - name: Timothy M Pollington
    email: timothy.pollington@gmail.com
    affiliation: [Big Data Institute, MathSys CDT]
  - name: Emma L Davis
    email: emma.davis@bdi.ox.ac.uk
    affiliation: Big Data Institute
  - name: T Déirdre Hollingsworth
    email: timcdlucas@gmail.com
    affiliation: Big Data Institute
address:
  - code: Big Data Institute
    address: Big Data Institute, Li Ka Shing Centre for Health Information and Discovery, University of Oxford, UK
  - code: MathSys CDT
    address: MathSys CDT, University of Warwick, UK
abstract: |
  Models are the keystone of infectious disease epidemiology and are increasingly reliant on computers rather than pen-and-paper methods. 
  Models of infectious disease have guided health policy for epidemics including COVID-19 and Ebola as well for endemic diseases such as malaria and tuberculosis. 
  that can affect millions of people. 
  Yet a single bug could bias results, leading to incorrect conclusions and wrong actions that could cause avoidable harm.
  We are ethically obliged to ensure our code is as free of error as possible.
  Software engineering already provides a solution in _unit testing_, a coding method that aims to avoid bugs in code.
  We demonstrate through simple examples how unit testing can handle the particular quirks of infectious disease models and encourage modellers to try it out in their next script. 
author_summary: |
bibliography: unit_testing.bib
output:
  bookdown::pdf_book:
    base_format: rticles::plos_article
    includes:
      in_header: preamble.tex
    number_sections: true
csl: plos.csl
---

```{r, setup, echo=FALSE, results = 'hide'}
rm(list = ls())
library(knitr)
oldSource <- knit_hooks$get("source")
knit_hooks$set(source = function(x, options) {
  x <- oldSource(x, options)
  x <- ifelse(!is.null(options$ref), paste0("\\label{", options$ref,"}", x), x)
  ifelse(!is.null(options$codecap), paste0("\\captionof{chunk}{", options$codecap,"}", x), x)
})
library(ggplot2)
library(testthat)
knitr::opts_chunk$set(cache = FALSE, fig.width = 7, fig.height = 4, 
                      out.extra = '', out.width = "0.6\\textwidth",
                      fig.align='center', fig.pos = "h")
set.seed(01011885)

# You might need to install the package rticles for the formatting to work.
# To build run
# rmarkdown::render('unit_testing.Rmd')
```
\newpage
# Introduction

Modelling is a hugely important tool in epidemiology as a way to understand fundamental biological processes, test intervention efficacy and forecast disease burden. 
For instance models are currently providing advice to governments on their response to the COVID-19 pandemic [@ihme; @imperial; @hellewell2020feasibility].
Similarly, policy on endemic infectious diseases is also directed by models TODO [an_ntd_paper_that_definitely_influenced_policy]. 
The decisions based on these models could significantly affect, to different degrees, the health outcomes of millions of people globally.

Given the importance of modelling results, it is vital that they are coded correctly as well as trusted.
The issue of trust was highlighted recently when Neil Ferguson, one of the leading modellers informing UK COVID-19 government policy, tweeted:

> "I'm conscious that lots of people would like to see and run the pandemic simulation code we are using to model control measures against COVID-19. To explain the background - I wrote the code (thousands of lines of undocumented C) 13+ years ago to model flu pandemics..." [@ferg_tweet].

The tweet garnered considerable backlash, with observers from the software industry noting that code should be both documented and tested to ensure its correctness.
Until the code is published we cannot comment on how it was tested. 
Bugs have been found in code used by many researchers [@bhandari2019characterization] or lead to retractions [@retract].
Outside of epidemiology, a paper that informed austerity policies globally was found to be fatally flawed [@] while in engineering bugs have caused spacecraft a Mars Climate orbiter and the Mariner 1 spacecraft to be lost or destroyed [@nasa, @mars].

However, the goal of this paper is not to single out any particular model or group, but instead to address a perceived chronic lack of rigorous testing across the field in general. 
Testing is currently either not implemented or not documented and reported, which reduces trust in model outputs, particularly during fast-moving epidemics with high public profile such as COVID-19.

Today's models are increasingly moving from mean-field ordinary differential equation approximations to individual-based models with complex, data-driven contact processes [@willem2017lessons; @ferguson2006strategies].
These increases in model complexity are accompanied with growing codebases. 
As the mathematical methods depend on numerical solutions rather than analytical pen-and-paper methods, it becomes increasingly difficult to tell when a bug is present based on model outputs.
<!--
I have a list of high impact bugs, but in summary a few highlights:
- Omission of a minus sign in 1995 cost Fidelity Magellan Fund £1.6million
- In 2013 a PhD student found basic mistakes in a globally influential economics study that was used by many countries to inform austerity
- London 2012 Olympics over-sold 10,000 synchronised swimming tickets
- 2010: MI5 incorrectly bugged over 100 phones due to a code formatting error
- Mars Climate orbiter lost in space due to failure to convert between imperial and metric units (1998)
- Mariner 1 space craft lost in 1962 due to omission of a hyphen
- Kerberas security system trivial to break for 8 years due to failure to correctly seed random number generator
--https://retractionwatch.com/2018/11/05/data-mishap-forces-retraction-of-paper-on-gun-laws-domestic-killings/#more-76505
https://retractionwatch.com/2020/05/05/doing-the-right-thing-researchers-retract-clinician-burnout-study-after-realizing-their-error/
https://retractionwatch.com/2016/09/26/error-in-one-line-of-code-sinks-2016-seer-cancer-study/
--->

The workflow followed by many is to write some code, run the full model and examine plots of the output.
This crude *ad hoc* testing approach can miss many bugs.
Furthermore, this workflow is biased as models that show expected behaviour are assumed bug-free, while models showing unexpected behaviour get more scrutiny.

_Unit testing_ is a formally-defined, principled framework that compares numerous output scenarios from code to what the programmer expected to happen (@wickham2015r Chapter 7, @osborne2014ten, @wilson2014best).
Ready-to-run frameworks for unit testing are available in \emph{R} [@R], \emph{Julia} [@bezanson2017julia] and \emph{python} [@python] and standard practice in the software industry.
These testing concepts also apply to ecology and other sciences but here we focus on infectious diseases.
Infectious disease modelling presents specific challenges, such as stochastic outputs, which are difficult to test and not covered in general unit testing literature.

In this primer we introduce unit testing and demonstrate its use on an example infectious disease model.
We also give an overview of the available testing frameworks in various languages commonly used by infectious disease modellers.

# Unit testing foundations
At the heart of every unit test is a function output, its known/expected value and a process to compare the two.
For the square root function $\sqrt{x}$ (`sqrt(x)` in \emph{R} [@R]), we would write a test that runs the function call on the number 4 i.e. `sqrt(x = 4)` and compares it to the correct answer i.e. 2.
Often our function arguments will cover an infinite range of possibilities and we cannot exhaustively check them all. 
Instead we devise tests that cover standard usage as well as _corner case_ scenarios: what do we want our function to do if given a negative number e.g. `sqrt(-1)`, or a vector argument containing strings or missing values e.g. `sqrt(c(4,"melon",NA))`?

In \emph{R}, the \texttt{testthat} package [@testthat], provides a simple interface for testing.
While a variety of test functions can make different comparisons, the two main ones are `expect_true()` and `expect_equal()`. 
`expect_true()` takes one argument: an expression that should evaluate to `TRUE`.
For our square root example above, we would write `expect_true(sqrt(4) == 2)`.
`expect_equal()` takes two arguments, an expression and the expected output; 
so we would write `expect_equal(sqrt(4), 2)`.

There are a number of ways to incorporate unit testing into your programming workflow.

1. As you write code, you should write tests that confirm it does what you expect.
2. These same tests should be regularly run as you develop new code.
If a change causes the older tests to break, this points to the introduction of an error in the new code, or implies that the older code could not generalise to the adapted environment.
3. Whenever a bug is found in the code, a test should be written that captures it. 
Then if the bug re-emerges it will be caught.
4. You can write tests before you write code---also known as test-driven development.
This can help development planning: write tests that define the desired functionality and continue programming, provided all tests keep passing.
Frameworks for formalising this are discussed in later in the paper.

# Bug in a multi-strain re-infection model

Here we define a simple epidemiological model and then demonstrate how to effectively write unit tests for it in \emph{R} code.

Consider a multi-strain system, with a population of $N$ infected individuals who each get reinfected at every time step.
Each individual is defined by the strain they are currently infected with $I_{it} \in \{a, b, c\}$ for a 3-strain system. So the the population is defined by a length $N$ state vector $\mathbf{I}_t$ = $(I_{it})_{i=[1,N]}$.
Each time step, everyone's infection status is updated as:
$$I_{it} = \text{Unif}(\mathbf{I}_{t-1}).$$
That is, at each iteration $t$, the new infection status of each individual $i$ is a Uniform random sample from the set of infection statuses in the previous iteration (including itself $I_{i,t-1}$).
Certainly this toy model is naïve as it is governed by mass-action principles, ignoring contact and spatial dynamics. 
Nevertheless it will serve its purpose. 
\@ref(firstcode) shows our first attempt at implementing this model.
\newline
```{r first_code,  ref = "firstcode", codecap = "Base example of the multi-strain re-infection model", results='hide'}
N <- 12 # infected individuals
t <- 20 # study length
# create the matrix to store the simulation data
I <- matrix(data = NA, nrow = t, ncol = N)

# Initialise the population at t = 1 with a fixed configuration
I[1,] <- rep(x = c("a", "b", "c"), length.out = N)

# At each time step, everyone is re-infected 
#   by someone from the previous time step.
for(t in seq(2, t)){
  I[t,] <- sample(x = I[t-1,], size = N)
}
```

Usually we would make some output plots to explore if our code is performing sensibly.
Plotting the time course of individuals' infection strain indicates repeated infection with different strains as expected (Fig. \@ref(fig:firstplots)).
However, if we look at the proportion of each strain through time (not shown here) we quickly see that the strain proportions are identical through time and so there must be a bug hiding.

This simple example demonstrates that:

* bugs can be subtle
* it is not easy to notice an error, even in just 7 lines of active code. Perhaps you can find it? The solution is in the next paragraph.
* it is much easier to debug code when you now there is a bug. 
* while plotting simulation runs are an excellent way to check model behaviour, if we had only relied on Fig. \@ref(fig:firstplots) we would have missed the bug. It is a generally ineffective method because a human has to perform this scan every test run.

In summary this *ad hoc* plotting approach reduces the chances that we will catch all bugs.

```{r, firstplots, echo = FALSE, fig.cap = 'Infection profile for individual 1.'}
library(ggplot2)
d1 <- data.frame(time = seq(t), strain = (I[, 1]), stringsAsFactors = TRUE)
ggplot(d1, aes(time, as.numeric(strain))) +
  geom_path() +
  geom_point() +
  scale_y_continuous(breaks = seq(3), labels = c('a', 'b', 'c')) +
  labs(y = 'Strain', x = 'Time') +
  theme(panel.grid.minor = element_blank())
```

```{r, bug_show_plot, include = FALSE}
apply(I, 1, function(x) table(factor(x, levels = c("a", "b", "c")))) %>% 
      t %>% matplot(type = 'l')
```

_Solution_: `sample()` defaults to sampling without replacement `sample(..., replace = FALSE)`; this means everyone swaps their infection strain on a one-to-one basis rather than one-to-many as required by the model.
Setting `replace = TRUE` fixes this and when we plot the proportion of each strain (Fig. \@ref(fig:correctplots)) we see the correct behaviour (one strain drifting to fixation).
We will now develop this example as we consider different concepts in unit testing, resulting in well-tested code by the end (\@ref(correctcode)). 
\newline
```{r, correctcode, ref = "correctcode", codecap = "Corrected base code", results = 'hide', echo = TRUE}
N <- 12
t <- 20

I <- matrix(data = NA, nrow = t, ncol = N)

I[1,] <- rep(x = c("a", "b", "c"), length.out = N)

for(t in seq(2, t)){
  I[t,] <- sample(x = I[t-1,], size = N, replace = TRUE)
}

```

```{r, correctplots, echo = FALSE, fig.cap = 'The correct behaviour with the proportion of each strain drifting. Each strain is a different line.'}
(apply(I, 1, function(x) table(factor(x, levels = c("a", "b", "c")))) / N) %>% 
  t %>% 
  matplot(type = 'l', xlab = 'Time', ylab = 'Proportion', lwd = 2)
```

# Basic testing
To ensure the unit test is evaluating the same code as the actual analysis, it needs to be defined once within a single function.
This same function is then used for the unit test in isolation or as part of the larger model codebase.

## Write compact functions {-#compactfuns}
Make your functions compact with a single clearly-defined operation.
We have defined a function `initialisePop()` to initialise the population and another `updatePop()` to run the simulation (\@ref(compactfunctions)).
Organising the codebase into these bite-sized operations makes following the programming flow easier as well as understanding the structure of the code.
We also modify `initialisePop(..., strains)` so that we can vary the number of strains.
\newline
```{r, compactfunctions, ref = "compactfunctions", codecap = "Organising code into more compact functions", result = 'hide'}
initialisePop <- function(t, N, strains){
  I <- matrix(data = NA, nrow = t, ncol = N)
  I[1,] <- rep(x = letters[1:strains], length.out = N)
  return(I)
}

updatePop <- function(x, t, N){
  x[t,] <- sample(x = x[t-1,], size = N, replace = TRUE)
  return(x)
}
```

## Test simple cases first {-#easycases}

If we start with a small population with few strains, we can then easily anticipate what we expect the initialised population to look like (\@ref(testsimplefirst)).
\newline
```{r, test_simple_first, ref = "testsimplefirst", codecap = "Using simple parameter sets we can work out beforehand what results to expect", results = 'hide'}
pop1 <- initialisePop(t = 2, N = 3, strains = 2) 
expect_equal(pop1[1,], c("a", "b", "a"))

pop2 <- initialisePop(t = 2, N = 3, strains = 3) 
expect_equal(pop2[1,], c("a", "b", "c"))

pop3 <- initialisePop(t = 2, N = 4, strains = 2) 
expect_equal(pop3[1,], c("a", "b", "a", "b"))
```

## Test all arguments {-#testargs}

`initialisePop()` has three arguments that require testing.
First we initialise the population, and then alter each argument in turn.
Arguments `t` and `N` directly change the expected dimension of matrix `I` so we check them together.
For the last argument we test that the number of strains is equal to the `strains` requested.
\newline
```{r, test_all_args, ref = "testallargs", codecap = "Test all function arguments in turn", results = 'hide'}
pop1 <- initialisePop(t = 2, N = 3, strains = 3) 
expect_equal(dim(pop1), c(2, 3))

pop2 <- initialisePop(t = 6, N = 3, strains = 3) 
expect_equal(dim(pop2), c(6, 3))

pop3 <- initialisePop(t = 2, N = 20, strains = 3) 
expect_equal(dim(pop3), c(2, 20))

pop4 <- initialisePop(t = 2, N = 10, strains = 5) 
expect_equal(length(unique(pop4[1,])), 5)
```

## Carefully work through more complex cases {-#complexcases}

We can also cover cases with more complex parameter values.
After initialising our population, we expect all the rows other than the first to contain `NA`.
We also expect each of the strains $a$, $b$ and $c$ to occur at least once on the first row if `strains` $\geq 3$ and `N` $\geq 3$.
Finally, `updatePop()` performs one simulation time step, so we expect one additional row to be populated.
Instead of testing the exact values of the results, we check they make sense within our macro understanding of the model system.
\newline
```{r, test_complex, ref = "testcomplex", codecap = "Test more complex cases using your understanding of the system", results = 'hide'}
pop1 <- initialisePop(t = 20, N = 12, strains = 3) 
# expect all except the first row are NAs
expect_true(all(is.na(pop1[-1,]))) 
# expect all 3 strains at t = 1
expect_true(all(c("a", "b", "c") %in% pop1[1,])) 

pop2 <- updatePop(pop1, t = 2, N = 12)
# after update expect 1st & 2nd row not to have NAs
expect_true(all(!is.na(pop2[1:2,]))) 
```

## Combine simple functions and test them at a higher-level{-#combine}

In the end an entire model only runs when its functions work together seamlessly. 
So we next check their connections; achieved through nesting functions together, or defining them at a higher level.
Developing at a higher level means we should check macro aspects of the model.
We now expect there to be no `NA`s left in matrix `I` and every element to be one of the three strain strings.
\newline
```{r, combine_simple_func, ref = "combinesimplefunc", codecap = "Combine simple functions through nesting to check higher-level functionality", result = 'hide'}
fullSim <- function(t, N, strains){
  pop <- initialisePop(t, N, strains) 
  for(i in seq(2, t)){
    pop <- updatePop(pop, i, N)
  }
  return(pop)
}

pop <- fullSim(t = 12, N = 20, strains = 3)
expect_true(!any(is.na(pop))) # expect no NAs
# expect all 3 strains contained within
expect_true(all(pop %in% c("a", "b", "c"))) 
```

# Stochastic code

Stochastic simulations are a common feature in infectious disease models.
Stochastic events are difficult to test effectively because, by definition, we do not know beforehand what the result will be.
We can check very broad-scale properties, like the test above, where we check the range of strain values.
However, code could still pass and be wrong (for example the original example would still pass that test).
There are however a number of approaches that can help.

## Split stochastic and deterministic parts {-#splitstochastic}

Isolate the stochastic parts of your code.
For example, `updatePop()` performs stochastic and deterministic operations in one line.
Firstly, `updatePop()` stochastically samples who gets infected by whom at iteration `t`.
Then it takes those infection events and assigns the new infectious status for each individual.
Finally, it adds the new infectious statuses to the full population matrix for that iteration.
So we could split these tasks up as follows.
\newline
```{r, split_deter_stoch, ref = "splitdeterstoch", codecap = "Isolation of the determistic and stochastic elements"}
chooseInfector <- function(N){
  sample(x = N, size = N, replace = TRUE)
}

newInfectionStatus <- function(old_status, infector_strain){
  old_status[infector_strain]
}

addInfectionStatus <- function(x, t, new_status){
  x[t,] <- new_status
}

updatePopCombined <- function(x, t, N){
  infector_strain <- chooseInfector(N)
  new_status <- newInfectionStatus(x[t-1,], infector_strain)
  x[t,] <- addInfectionStatus(x, t, new_status)
  return(x)
}
```

Two-thirds of `updatePopCombined()` is deterministic so can be checked using the ideas discussed previously.
However, we still have `chooseInfector()`, that is irreducibly stochastic.

## Pick a smart parameter for a deterministic result{-#deterministicparams}

In the same way that we used simple parameters previously, we can often find parameter values for which our stochastic functions become deterministic.
For example, samples from $X\sim\text{Bernoulli}(p)$ always give zeroes for $p=0$ or ones for $p=1$.
In the case here where we only have one strain, the model is no longer stochastic.
So initialisation with one strain means the second time step should equal the first.
\newline
```{r, test_stoch_determin, ref = "test_stoch_determin", codecap = "A stochastic function can output deterministically if you can find the right parameter set.", results = 'hide'}
pop <- initialisePop(t = 2, N = 3, strains = 1) 
pop <- updatePop(pop, t = 2, N = 3)
expect_equal(pop[1,], pop[2,])
```

## Test all possible answers (if few) {-#allpossible}

Working again with a simple parameter set, there are some cases where the code is stochastic, but with a small, finite set of outputs. 
So we can run the function exhaustively and check it returns all of the possible outputs.
For a population of two people, there are four possibilities when drawing who is infected by whom.
Individual 1 can be infected by themselves ({1,}) or individual 2 ({2,}), and individual 2 infected by themselves ({,2}) or individual 1 ({,1}).
\newline
```{r, test_stoch_fewvalues, ref = "teststochfewvalues", codecap = "Testing stochastic output when it only covers a few finite values", results = 'hide'}
# Collapse each draw into a single string to make comparisons easier.
manyPops <- replicate(300, paste0(chooseInfector(N = 2), collapse = ""))
expect_true(all(manyPops %in% c("11", "12", "21", "22")))
```

## Use very large samples for the stochastic part {-#largesamples}

While the previous example worked well for a small set of possible outputs, testing can conversely be made easier by using very large numbers.
This typically involves large sample sizes or numbers of stochastic runs.
For example, the clearest test to distinguish between our original, buggy code and our correct code is that in the correct code there is the possibility for one individual to infect more than one in a single time step.
In any given run this is never guaranteed, but the larger the population size (and equivalently the sample size) the more likely it is to occur. 
Though we need to be sure that the failure of the test is very rare. 
We set the seed of a random number generator so that the test is reproducible, otherwise the test will fail sporadically and unpredicatably.

We can calculate the probability of this test failing for a population with $n$ individuals---it is the well-known 'birthday paradox'.
There are '$n$ choose 2' $\binom{n}{2}$ possible pairs of individuals.
The probability of a pair $<i,j>$ not being infected by the other is $1-\frac{1}{n}$ (given that the first person is infected by someone $i$, the probability that the second person $j$ is infected by the same $i$ is just $1/n$).
For the test to fail, every pair would have to not be infected by the other, giving $\big(1-\frac{1}{n}\big) ^ {\binom{n}{2}}$.
For a population with 100 individuals, the probability of the test failing (if programmed correctly) is $10^{-21}$ and vanishingly small for 1000 individuals.
\newline
```{r, test_stoch_largenum, ref = "teststochlargenum", codecap = "Computing the probability of the stochastic test failing for 1000 individuals", results='hide'}
set.seed(10261985)
n <- 1e3
infector_strain <- chooseInfector(n)
expect_true(any(duplicated(infector_strain)))

(1 - (1 / n)) ^ choose(n, 2) # probability of failing
```

If we have an event that we know should not happen, we can use a large number of simulations to provide stronger evidence that is does not stochastically occur.
However, it can be difficult to determine how many times is reasonable to run a simulation, especially if time is short.
This strategy works best when we have a specific bug that occurred relatively frequently (perhaps once every ten simulations or so).
We say that if the bug does not occur even once in 500 simulations, then we are pretty sure we have fixed it.
In our original buggy code we found that the proportions remained identical for entire simulations.
For a reasonably large simulation we would expect this to seldom happen, so we can run short simulations to check that this specific bug does not occur by confirming that the proportion of each strain is not always the same between the first and last time point.
\newline
```{r, returningstrain,ref = "returningstrain", codecap = "Assessing if a bug fix was a likely success with large code runs, when the bug was appearing relatively frequently"}
set.seed(11121955)
manySims <- replicate(500, fullSim(t = 20, N = 40, strains = 3), 
                      simplify = FALSE)

# Define a function that calculates whether
#   whether the strain proportions are the
#   same in the first and last time point.
diffProportions <- function(x){
  !identical(table(x[1, ]), table(x[20, ]))
}

# Check that at least one simulation had 
# non-identical proportions.
expect_true(any(sapply(manySims, diffProportions)))
```

# Further testing
## Test edge cases and special cases {-#corners}

When writing tests it is easy to focus on standard behaviour.
However, bugs often occur at _edge cases_---when parameters are at their extrema or at special values.

For example, in \emph{R}, selecting one column from a matrix e.g. `[,2]` returns a vector while selecting two or more columns e.g. `[,2:3]` returns a matrix.
Code that relies on the returned object being a matrix would fail this vector edge case.

Equally special cases can be triggered with parameter sets that do not match the extrema of parameter space. 
This is where understanding of the functional form of the model can help. Consider a function `divide(x, y)` that divides `x` by `y`. 
If we test the (mathematically bloated) calculation `2 * divide(x, 2) == x` we would believe it works for nearly all values of division, unless we ever try `y = 0`.

In our example model, we might consider testing edge cases such as one individual, one iteration and one strain.
```{r, edge, eval = FALSE}
pop <- fullSim(t = 1, N = 12)
```

In fact, running `fullSim()` with only one iteration, errors without a useful error message.
We have fixed this with an error check at the top of the function that returns a more useful error message if `t = 1` (or any integer less than two) is used.
\newline
```{r, erroronspecialcase, ref = "erroronspecialcase", codecap = "Testing a special case", eval = FALSE}
fullSim2 <- function(t, N, strains){
  stopifnot(t > 1)
  pop <- initialisePop(t, N, strains) 
  for(i in seq(2, t)){
    pop <- updatePop(pop, i, N)
  }
  return(pop)
}
pop <- fullSim2(t = 1, N = 12, strains = 3)
```

Similarly, we checked earlier if the `strains` argument of `initialisePop()` worked by verifying if the returned population had the correct number of strains.
However, if we set the `strains` argument to be greater than the number of individuals in the population, the current function does not error nor give the answer we expected either.
\newline
```{r, edge2, ref = "edge2", codecap = "Sometimes the best way to avoid bugs it using adequate error checks on variables at the function start", eval = FALSE}
pop <- initialisePop(t = 10, N = 2, strains = 11) 
expect_equal(length(unique(pop[1,])), 11)
```

Here again the best solution is a `stopifnot(strains > N)` error check at the beginning of the function.

## Test incorrect inputs {-#testincorrect}

Bugs may be discovered by other users when new uncleaned data meets functions previously tested on complete datasets. 
Therefore it is useful to test that functions fail gracefully if they are given inputs of the wrong class.
This is especially true for 'external' functions, available to a user on a package's front-end.
Some cases may quickly fail, but sometimes code may run silently giving false results or take long to reach failure.
Both of these cases are serious or annoying and can be difficult to debug afterwards.

A common case to check with epidemiological datasets is whether code correctly handles missing data (normally represented as `NA`, `" "` or `''`). 
Often the simplest solution is to include argument checks at the beginning of functions and use `expect_error()` to error check.

We check how the code handles a user inputting a vector of numbers to the `t` argument (perhaps thinking it needed a vector of all time points to run).
We have also updated `fullSim3()` to provide a check on the length of `t` with an informative message.
\newline
```{r, wrong1, ref = "wrong1", codecap = "Testing incorrect inputs"}
expect_error(pop <- fullSim(t = 1:100, N = 4))

fullSim3 <- function(t, N, strains){
  stopifnot(t > 1)
  stopifnot(length(t) == 1)
  pop <- initialisePop(t, N, strains) 
  for(i in seq(2, t)){
    pop <- updatePop(pop, i, N)
  }
  return(pop)
}
```

# Unit testing frameworks {#frameworks}

Most programming languages have established testing packages.
For \emph{R}, there is the $\texttt{testthat}$ package.
When structuring \emph{R} code as a package, tests should be kept in the directory `tests/testthat`; further requirements to the structure can be found in @wickham2015r Chapter 7.
All the tests in a package can then be run with `test()` from the $\texttt{devtools}$ package [@devtools] or `check()` for additional checks relevant to a package build.
If the code is to be part of a package then these tools are essential to run the code within the context of a build environment.
These tools also provide a clean environment to highlight if a test was previously relying on objects defined outside of the test script.

Other programming languages have similar testing frameworks too.
Their specifics differ but the main concept of comparing a function evaluation to the expected output remains the same.
In \emph{Julia} there is the $\texttt{Test}$ package [@juliatest]. 
The basic structure for tests with this package are demonstrated below.
We give the specific test a name and give a single expression that evaluates to `TRUE` or `FALSE`.
For a \emph{Julia} package unit tests reside in `test/runtests.jl` and tests are run with `Pkg.test()`.
\newline
```{r, juliatest, ref = "juliatest", codecap = "Julia test example", eval = FALSE}
@testset "my_test_name" begin
  @test sqrt(4) == 2
end
```

Finally, in \emph{python} we have the $\texttt{unittest}$ framework [@pythonunittest]; tests must be written into a class that inherits from the `TestCase` class.
The tests must be written as methods with `self` as the first argument.
An example test script is shown below.
Tests should be kept in a directory called `Lib/test`, and the filename of every file with tests should begin with `test_`.
\newline
```{r, testpy, ref = "testpy", codecap = "python test example", eval = FALSE}
import unittest
import math

class TestMaths(unittest.TestCase):
    def test_sqrt(self):
        self.assertEqual(math.sqrt(4), 2)

unittest.main()
```

# Continuous integration {#ci}

If your code is under version control [@osborne2014ten; @wilson2014best] and hosted on GitHub, GitLab or BitBucket, you can automate the running of unit tests---also known as _continuous integration_.
In this setup, whenever you push code changes from your local computer to the online repository, any tests that you have defined get run automatically.
Furthermore these tests can be automated irrespective of changes to your code or tests: since dependencies with other packages and knock-on changes from them into a bug in your code can be automatically found and notified to you by email.
There are various continuous integration services such as travis-ci.org, GitHub actions and GitLab pipelines.
These services are often free on a limited basis, or free if your code is open source.

We briefly describe the setup of the simplest case.
For \emph{R} code organised into a package and hosted openly on GitHub, then setting up travis CI is very straightforward.
Within your version-controlled folder you add a one-liner `.travis.yml` file that contains a description of which language the code uses.

```{awk, travis, eval = FALSE}
language:r
```

This file can also be created using `use_travis()` from the $\texttt{usethis}$ package.
You then need to sign up to travis-ci.org and point it to the correct GitHub repository.
To authenticate and trigger the first automatic test you need to make a minor change to your code, commit and push to GitHub.
More details can be found in @wickham2015r Chapter 14.3.

# Conclusions

It is vital that infectious disease models are coded to minimise bugs.
Unit testing is the principled way we can do this.
There are many frameworks that make aspects of testing automatic and more informative and these should be used where possible.
Setting it up is simple but writing good tests is a skill that takes time, practice and thought, but also saves time, effort and unintended harm from incorrect code.

# Code availability

Please see the fully-reproducible and version-controlled code at \url{www.github.com/timcdlucas/unit_test_for_infectious_disease}.

# References {#references .unnumbered}
