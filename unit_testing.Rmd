---
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: false
    keep_tex: true
title: "Unit testing for infectious disease modelling"
author: Tim CD Lucas, Tim Pollington, Deirdre Hollingsworth, et al.
geometry: margin=1in
bibliography: unit_testing.bib
---


<!--
To build run
rmarkdown::render('unit_testing.Rmd')
-->



<!--
Title: A beginners guide to unit testing for infectious disease epidemiologists
Title: A guide for effective unit testing for robust, infectious disease code.
-->


<!--
# use covid Ferguson as motivating example.
# https://twitter.com/neil_ferguson/status/1241835454707699713?s=19


# good practice
#.   small functions

# what to test
#.  edge cases. 0, others?
  #.  types
#.  dims
#.  bounds


# how to test random
#.   non random pars. p= 0
#.   separate the random bit
#.   large numbers and bounds
#.   Test for rare events again with large number


# frameworks
#.  testthat and packages
#.  test in Julia
#.  add python

# continuous integration
-->


```{r, setup, echo = FALSE, results = 'hide'}
library(ggplot2)
library(testthat)
knitr::opts_chunk$set(cache = FALSE, fig.width = 7, fig.height = 4, 
                      out.extra = '', out.width = "0.6\\textwidth",
                      fig.align='center')
```

# Abstract


Models are a core part of infectious disease epidemiology and are increasingly computations rather than pen-and-paper mathematical.
Noone writes guarenteed bug-free code.
However, models of infectious diseases are used to guide policy that determine the health, morbidity or mortality of billions of people.
It is therefore critical that the code we write is as correct as we can possibly make it.
Furthermore, given that bugs are more readily found when they give unexpected results, buggy code is biased code.
Within software engineering, the core method for reducing the number of bugs in code is unit testing.
Unit testing is a formally defined framework that compares the outputs from code to what the programmer was expecting the code to produce.
In this article we introduce unit testing, demonstrate how it can handle the particular difficulties of infectious disease models and encourage all infectious disease biologists to start unit testing their code.


# Introduction

Modelling is a hugely important tool in epidemiology as a way to forecast disease burdens, test the effectiveness of interventions and to understand fundemental biological processes.
These models are commonly used to guide policy for the control of endemic disease and also during fast moving epidemics and pandemics.
Examples include the models being used to directly adviae govenments on responses to the COVID-19 pandemic [@ihme; @imperial; @hellewell2020feasibility] and [an_ntd_paper_that_definitely_influenced_policy].
The results of these models therefore directly cause or prevent many deaths and influence the morbidity of millions of people globally.
The models being used are increasingly moving from mean-field ordinary differential equations to individual based models and models with complex, data driven contact processes  [@willem2017lessons; @ferguson2006strategies].
This increase in model complexity is accompanied by an increased reliance on code to implement our models.
The code bases for these models are often complex and large.

<!--
Iâ€™m conscious that lots of people would like to see and run the pandemic simulation code we are using to model control measures against COVID-19.  To explain the background - I wrote the code (thousands of lines of undocumented C) 13+ years ago to model flu pandemics...
https://twitter.com/neil_ferguson/status/1241835454707699713
--->

Given the importance of modelling results, it is vital that they are both correct and trusted.
The trust issue was highlighted during the COVID-19 pandemic when Neil Ferguson, one of the leading modellers informing UK government policy, tweeted
"To explain the background - I wrote the code (thousands of lines of undocumented C) 13+ years ago to model flu pandemics" [@ferg_tweet].
The tweet garnered considerable backlash with observers from the software industry noting that code should be both documented and tested to ensure its correctness.
The workflow followed by many epidemiologists is to write some code, run some full models and examine plots of the output.
This *ad hoc* testing is not very sensitive and can miss many bugs.
Furthermore, this workflow is biased as models that show "expected behaviour" are taken to be bug free while models showing unexpected behaviour are more thoroughly scrutinised.

Unit testing is a framework for testing code that is more principled (@wickham2015r Chapter 7, @osborne2014ten, @wilson2014best).
Useful frameworks for unit testing are readily available in R [@R], Julia [@bezanson2017julia] and python [@python].
While unit testing is standard practice in the software industry, infectious disease modelling presents some specific challenges, such as stochastic code, which are difficult to test and not covered in more general introductions to unit testing.
In this primer we introduce unit testing and demonstrate its use on an example infectious disease model.
We give special focus to recommendations for the problems specific to infectious disease modelling.
We also give an overview of the testing frameworks available.


# Unit testing basics


Unit testing consists of pairs of function calls and known/expected outputs.
So far a function that computes the square root of a number, we would write a test that runs our function on the number 4 and compares it to the known, correct answer (2).
We would write tests that cover various possible inputs: what do we want our function to do if it is given a negative number, or a string instead of a numeric object?

In R [@R], the testthat package [@testthat], provides a simple interface for testing.
While there are a number of test functions, the two main functions is `expect_true` and `expect_equal`. 
`expect_true` takes one argument, an expression that should evaluate to true.
So for our square root example above we would write `expect_true(sqrt(4) == 2)`.
`expect_equal` takes two arguments, an expression and the expected output.
For the square root example we would write `expect_equal(sqrt(4), 2)`.

There are a number of ways that unit testing can be incorporated into programming workflow.
The first and clearest is that as you write code, you should write tests that test the code does what you think it does.
The second, is that these same tests should then be regularly run as you continue to develop code.
If a change elsewhere in the code causes the older tests to break, something has gone wrong and needs to be fixed.
Finally, you can write tests before you write code.
This can be a helpful way to force yourself to plan what some new code or functionality is expected to do.
Write tests that define the wanted functionality, and keep on programming until all the tests pass.
This is known as test-driven development.
Frameworks for formalising these frameworks are discussed in Sections [6](frameworks) and [7](ci).



# Example

<!--
# this example should
#  demonstrate that bugs can be subtle
#  Have elements that are fixable by all the ideas later
-->

Here we define a simple epidemiological model and then present R code for it.
We will then use this as a working example to demonstrate how to effectively write unit tests for infectious disease modelling code.

Consider a multistrain system, with a population of $N$ individuals who eacg get reinfected exactly once per time step.
Each individual is defined by the strain they are currently infected with $I_{it} \in \{a, b, c\}$ and so the the population is defined by a length $N$ vector of states $\mathbf{I}_t$.
Each time step, the infection status of each individual is updated as
$$I_{it} = \text{Unif}(\mathbf{I}_{t-1}).$$
That is, each iteration, the new infection status of each individual is a uniform random sample from the set of infection statuses in the previous iterations (including individual $I_i$).
Here then is our first attempt at implementing this model.

```{r, simsetup, results = 'hide', echa = FALSE}
set.seed(3132)
```


```{r, first_code, results = 'hide'}
N <- 12
t <- 20
# create the matrix to store the simulation data
I <- matrix(NA, ncol = N, nrow = t)

# Initialise the population
I[1, ] <- rep(c("a", "b", "c"), N / 3)

# For each time step, all N individuals are reinfected
#   by someone randomly sampled from the population at t-1.
for(t in seq(2, t)){
  I[t, ] <- sample(I[t - 1, ], N)
}
```

We might then make some plots to explore whether our code is doing something sensible.
For example we might plot the time course of one or more individuals and see that they are being repeatedly infected with different strains as expected (Figure \@ref(fig:firstplots)).
However, if we look at the proportion of each strain through time (not shown here) we will quickly see that the proportions are identical through time.
Therefore there must be a bug in our code.

This simple example demonstrates a number of points.
Firstly, as everyone who has ever written code knows, bugs can be subtle.
Even though the code above is only a few lines, it is not easy to notice that it is wrong.
Secondly, it is much easier to debug code, once you know there is a bug.
Having now read that there is a bug in the above code, a good proportion of readers might be able to identify the bug.
Thirdly, testing the code with a few plots is not very dissimilar from the testing that many scientists do; write the code, plot some outputs and see if they make sense.
However, approaching this in an ad hoc way makes it less likely that we will catch bugs.
In this example, we initially used the wrong plot to diagnose whether our code was correct and therefore missed the bug.
Finally, while plots are an excellent way to check the behaviour of code, and plotting simulation runs is a vital step in code checking, it is not the easiest method for automatically checking code because a human has to scan each plot each time the tests are run.



```{r, firstplots, echo = FALSE, fig.cap = 'The infection status of one individual though time.'}
d1 <- data.frame(time = seq(t), strain = I[, 1])
d1$strain_num <- as.numeric(factor(d1$strain))
ggplot(d1, aes(time, strain_num)) + 
  geom_path() +
  geom_point() +
  scale_y_continuous(breaks = seq(3), labels = c('a', 'b', 'c')) +
  labs(y = 'Strain', x = 'Time')
```


```{r, firstplots2, echo = FALSE, result = 'hide', include = FALSE}
d1 <- data.frame(time = rep(seq(t), 3), 
                 strain = as.vector(I[, 1:3]),
                 individual = factor(rep(1:3, each = t)))
d1$strain_num <- as.numeric(factor(d1$strain))
ggplot(d1, aes(time, strain_num, colour = individual, group = individual)) + 
  geom_path() +
  scale_y_continuous(breaks = seq(3), labels = c('a', 'b', 'c')) +
  labs(y = 'Strain', x = 'Time', colour = 'Indiv.')
```


```{r, bug_show_plot, include = FALSE}
apply(I, 1, function(x) table(factor(x, levels = c("a", "b", "c")))) %>% t %>% matplot(type = 'l')
```

The cause of this particular bug is that the `sample` function defaults to sampling without replacement.
Setting `replace = TRUE` fixes the code and when we plot the proportion of each strain (Figure \@ref(fig:correctplots)) we now see the correct behaviour.
Given this example, we will now work through specific advice for unit testing for infectious disease models.
We will write tests and rewrite the code as we go until we have a well tested, more reliabley implemented model at the end.

```{r, correct_code, results = 'hide', echo = FALSE}
N <- 12
t <- 20
I <- matrix(NA, ncol = N, nrow = t)
I[1, ] <- rep(c("a", "b", "c"), N / 3)

for(i in seq(2, t)){
  I[i, ] <- sample(I[i - 1, ], N, replace = TRUE)
}
```



```{r, correctplots, echo = FALSE, fig.cap = 'The correct behaviour with the proportion of each strain drifting. Each strain is a different line.'}
(apply(I, 1, function(x) table(factor(x, levels = c("a", "b", "c")))) / N) %>% 
  t %>% 
  matplot(type = 'l', xlab = 'Time', ylab = 'Proportion', lwd = 2)
```


# Tips and best practice

## Basic testing
### Write small functions {-#smallfuns}

In order to guarantee that a unit test is checking the same code as used for actual analysis, the code needs to be defined once, in a function.
This same function is then used in the larger model code and used by the unit test itself.
It helps to make your functions small, such that they do one, clearly defined operation.
This makes it easier to reason through a few examples to be confident you know exactly what the output should be.
In our example we could define one function to initialise the population and another to run the simulation.
In doing so we can more carefully consider the structure of our code more generally.
Here we have modified the code that initialises the population so that we can vary the number of strains and so that the number of individuals does not have to be a multiple of the number of strains.

```{r, refactor, result = 'hide'}
initialise_pop <- function(N, t, strains = 3){
  I <- matrix(NA, ncol = N, nrow = t)
  I[1, ] <- rep(letters[1:strains], ceiling(N / strains))[1:N]
  return(I)
}

update_pop <- function(x, N, i){
  x[i, ] <- sample(x[i - 1, ], N, replace = TRUE)
  return(x)
}
```



### Test easy to understand parameters {-#easyparams}

In order to carefully work out exactly what we expect each function to do, it is useful to use easy to understand parameters.
For example, if we set up a very small populations with only a few strains we can easily work out exactly what we expect the initialised population to look like.


```{r, test_easy, results = 'hide'}
pop1 <- initialise_pop(3, 2, strain = 2) 
expect_equal(pop1[1, ], c('a', 'b', 'a'))

pop2 <- initialise_pop(3, 2, strain = 3) 
expect_equal(pop2[1, ], c('a', 'b', 'c'))

pop3 <- initialise_pop(4, 2, strain = 2) 
expect_equal(pop3[1, ], c('a', 'b', 'a', 'b'))
```

### Test all arguments {-#testargs}

Our function `initialise_pop` has three arguments so we should test that each of these does what we expect.
To do this we will initialise one population, and then alter each argument in turn.
The first two arguments directly change the expected dimension of the output matrix, so we test that.
For the last argument we have tested that the number of strains is equal to the number of strains requested.

```{r, test_all_args, results = 'hide'}
pop <- initialise_pop(3, 2, strain = 3) 
expect_equal(dim(pop), c(2, 3))

pop2 <- initialise_pop(3, 6, strain = 3) 
expect_equal(dim(pop2), c(6, 3))

pop3 <- initialise_pop(20, 2, strain = 3) 
expect_equal(dim(pop3), c(2, 20))

pop4 <- initialise_pop(10, 2, strain = 5) 
expect_equal(length(unique(pop4[1, ])), 5)

```



### Carefully work through more complex parameters {-#complexparams}

We can also write tests for each function with less simple parameter values.
After initialising our population, we expect all the rows other than the first to contain NAs.
We also expect each of the strains a, b and c to occur in the first row.
Our function update performs one time step of the simulation, so we expect one additional row to be populated.
Instead of testing the exact values of the results here, we are testing that they make sense with our more macro understanding of the model system.

```{r, test_complex, results = 'hide'}
pop <- initialise_pop(12, 20) 
expect_true(all(is.na(pop[-1, ])))
expect_true(all(c('a', 'b', 'c') %in% pop[1, ]))

pop2 <- update_pop(pop, 12, 2)
expect_true(all(!is.na(pop2[1:2, ])))
```


### Combine smaller functions and test the upper-level functions {-#combine}

There are many aspects of code that are best tested using small, specific functions.
However, we also need to test that we are tying these functions together properly.
Therefore, we should write larger functions that can also be tested.

```{r, refactor2, result = 'hide'}

full_sim <- function(N, t){
  pop <- initialise_pop(N, t) 
  for(i in seq(2, t)){
    pop <- update_pop(pop, N, i)
  }
  return(pop)
}

```

Where possible we should test larger, more macro aspects of the model here.
We now expect there to be no NAs left in our matrix.
We also now expect every element in our matrix to be one of the three strain strings.

```{r, testnn, results = 'hide'}
pop <- full_sim(12, 20)
expect_true(!any(is.na(pop)))
expect_true(all(pop %in% c('a', 'b', 'c')))
```



## Testing stochastic code

One of the most particular challenges for testing code for infectious disease modelling is the large amount of stochastic events.
Stochastic events are difficult to test effectively because, by definition, we do not know exactly what the result will be.
There are however a number of approaches that can help.

### Split stochastic and deterministic parts {-#splitstochastic}

The first thing to aim for is to isolate the stochastic parts of the code.
For example, the function `update_pop` above does a number of things, some of which are stochastic and some of which are deterministi.
Firslty, the function stochastically samples who gets infected by whom.
Then it takes those infection events and finds the new infectious status for each individual.
Finally, it adds the new infectious statuses to the full population matrix. 
So we could split these tasks up as follows.

```{r, refactorstoch}

sample_events <- function(N){
  sample(N, N, replace = TRUE)
}

new_infection_status <- function(old_i, events){
  old_i[events]
}

add_infection_status <- function(x, new_i, i){
  x[i, ] <- new_i
}

update_pop <- function(x, N, i){
  events <- sample_events(N)
  new_i <- new_infection_status(x[i - 1, ], events)
  x[i, ] <- add_infection_status(x, new_i, i)
  return(x)
}

```

Now two thirds of the code is deterministic and can be tested using the ideas discussed previously.
Similarly, the function `update_pop` that combines these functions can be tested at a more macro level, confident that two thirds of the code inside it is already tested very carefully.
However, we still have one function, `sample_events`, that is irreducably stochastic.


### Test deterministic parameters for stochastic code {-#deterministicparams}

Similarly to finding parameters that are simple to test for other functions, we can often find parameter values for which our stochastic functions are not longer stochastic.
For example, a Bernoulli sample with probability 0 always gives a 0 and with a probability of 1 always gives a 1.
In the example here, in the case where we only have one strain, the model is no longer stochastic.
So if we initialise a population with one strain, the second time step should equal the first time step.

```{r, test_stoch_determin, results = 'hide'}
pop <- initialise_pop(3, 2, strain = 1) 
pop <- update_pop(pop, 3, 2)
expect_equal(pop[1, ], pop[2, ])
```

### Test all possible answers {-#allpossible}

Again foussing on very simple parameters, there are some cases where the code is stochastic, but has a very small number of possible outputs. 
In this case we can run the function many times and check that it always gives one of the possible outputs.
In the case of a population of two people, there are four possible outcomes when drawing who is infected by whom.
Individual 1 can be infected by individual 1 or individual 2 and individual 2 an be infected by individual 1 or individual 2.


```{r, test_stoch_fewvalues, results = 'hide'}
# Collapse each draw into a single string to make comparisons easier.
many_pops <- replicate(300, paste0(sample_events(2), collapse = ''))
expect_true(all(many_pops %in% c('11', '12', '21', '22')))
```


### Use very large samples for stochastic part {-#largesmaples}

While the previous example looked at very small numbers, we can also consider how testing is made easier by using very large numbers.
This typically includes either using large sample sizes or large numbers of runs of a stochastic part.
For example, the clearest test to distinguish between our first-attempt, buggy code and our correct code is that in the correct code there is the possibility for one individual to infect more than one individuals in a time step.
In any given run however, this is never guarenteed, but the larger the population size the more likely it is to occur.
So we can use a large enough sample size that this is almost certain to occur (but make sure we set the seed so that the test is reproducible and does not stochastically fail every now and then).
We can work out the probability of this test failing for a population with $P$ individuals (it is the well known birthday paradox).
There are $P$ choose 2 possible pairs of individuals (also written as $P \choose 2$.
The probability of a pair of people not being infected by the same person is $1 - 1/P$ (given that the first person is infected by person $a$, the probability that the second person is infected by $a$ is just $1/P$).
For the test to fail, every pair would have to not be infected by the same person giving $(1 - 1/P) ^ {P \choose 2}$.
For a population with 100 individuals, the probability of the test failing (if programmed correctly) is $10^{-21}$ and is vanishingly small for 1000 individuals.


```{r, test_stoch_largenum}
set.seed(20200406)
P <- 1e3
events <- sample_events(1e6)
expect_true(max(table(events)) > 1)

prob_of_failing <- (1 - (1 / P)) ^ choose(P, 2)
```

If we have an event that we know should not happen, we can use a large number of simulations to provide stronger evidence that is does not stochastically occur.
However, it can be difficult to determine how many times is reasonable to run a simulation especially as we want unit tests to run quickly.
This strategy works best when we have a specific bug that occurred relatively frequently (perhaps in one in ten simulations).
We can then say that if it does not occur in 500 simulations we are pretty sure we fixed the bug.
In our original buggy code we found that the proportions remained identical for entire simulations.
For a reasonably large simulation we would expect this to happen very rarely, so we can run the full simulation a number of times to check that this specific bug does not occur.

```{r, returningstrain}
set.seed(20200407)
many_sims <- replicate(100, full_sim(40, 30), simplify = FALSE)

expect_true(all(sapply(many_sims, function(x) length(unique(apply(x, 1, table))) > 1)))

```

## Further testing
### Test edge cases and special cases {-#corners}


When writing tests it is easy to focus on the standard behaviours.
However, bugs often occur when parameters are at the edges of their values or for certain special values.
For example, in R, selecting one column from a matrix returns a vector while selecting two or more columns returns a matrix.
Code that relies on the returned object being a matrix will often fail in the edge case of selecting one column.

Similarly, in many programming applications, there are many input values for which there is a clear pattern between input and output but a few input values that do not fit the clear pattern, even when these are not at the extremes of the possible parameter range.
For example, consider a function `div(x, y)` that divides `x` by `y`. 
If we test that `2 * div(10, 2) == 10` this suggests that the function works for nearly all values of division.
However, this test will fail for $y = 0$.
We call cases like these special cases.

In our example model, we might consider testing edge cases such as one individual, one iteration and one strain.
In fact, running `full_sim` with only one iteration errors without a useful error message.
In practice we would probably fix this with a check at the top of the function that returns a useful error message if the user tries to run the function with `t = 1`.

```{r, edge, eval = FALSE}
pop <- full_sim(12, 1)
```

Similarly, early we tested whether the `strain` argument of `initialise_pop` worked by checking whether the returned population had the correct number of strains.
However, if we set the `strain` argument to be greater than the number of individuals in the population, the function we have currently written does not error but does not give the answer we expected.
Here again there is probably no solution other than a check at the beginning of the function that returns an error if `strain` is larger than `N`.

```{r, edge2, eval = FALSE}
pop <- initialise_pop(10, 2, strain = 11) 
expect_equal(length(unique(pop[1, ])), 11)
```



### Test incorrect inputs {-#testincorrect}

The arguments examined in the edge and special case section could be considered incorrect.
But there are many other ways that a user may input incorrect arguments.
Therefore it is useful to test that functions fail gracefully if they are given inputs of the wrong class.
This is especially true for functions that a user might use directly.
Some of these cases will quickly fail, but sometimes a function can either silently give wrong results or run a long time before erroring.
Both of these cases are dangerous or annoying and an be difficult to debug after the fact.
A common case to check, that is can be common in infectious disease modelling, is testing whether code handles data with NAs in correctly. 
Code will often be written with a clean, complete dataset. 
When it comes for the code to be reused, the new data is often incomplete and this an cause difficult to diagnose bugs.
Often the simplest solutions to these sorts of issues is to include code at the beginning of the function that checks the arguments are as expected.
We can write tests that expect an error to make sure we have handled these cases correctly.

Here we demonstrate one example with the code above.
We test how the code handles a user inputting a vector of numbers to the `t` argument (perhaps thinking the function requires a vector of all time points to run).
In this case the code does error, though perhaps we could make the function return an error message that tells the user how to fix the problem (i.e. tell the user that t should be of length 1).

```{r, wrong1}
expect_error(pop <- full_sim(4, 1:100))
```



# Unit testing frameworks {#frameworks}

While unit tests can be written with functions that throw errors, most programming languages have ready  built testing functions that can give useful information when a test fails.
In R, as described above, there is the testthat package.

When structuring R code as a package, code should be kept in the directory `tests/testthat`.
Further requirements to the structure can be found in @wickham2015r Chapter 7.
All the tests in a package can then be run with the function `test` from the testthat package but also by running `check` from the devtools package [@devtools] which also check a number aspects of the package.
Running tests within a package like this has the added benefit that the tests are run in a new, clean environment.
One easy-to-make mistake when testing is for a test to rely on an object that is not defined in the test script.
Running the tests in a clean environment quickly highlights these errors so they can be fixed.

Other programming languages have similar testing frameworks.
The specifics of the frameworks in the various programming languages differ but the main concept of defining an expression to run and an expected output stays the same.
In Julia there is the Test package [@juliatest]. 
The basic structure for tests with this package are demonstrated below.
We give the specific test a name and give a single expression that evaluates to true or false.
Unit tests within a Julia package live in the file `test/runtests.jl` and the tests can be run with the function `Pkg.test`.

```{julia, test, eval = FALSE}
@testset "my_test_name" begin
    @test sqrt(4) == 2
end
```

Finally, in python we have the unittest framework [@pythonunittest].
In this framework, tests must be written into a class that inherits from the `TestCase` class.
The tests must be written as methods with `self` as the first argument.
An example test script is shown below.
Tests should be kept in  a directory called `Lib/test`, and the filename of every file with tests should begin with `test_`.

```{python, testpy, eval = FALSE}
import unittest
import math

class TestMaths(unittest.TestCase):
  
  def test_sqrt(self):
    self.assertEqual(math.sqrt(4), 2)

unittest.main()
```



# Continuous integration {#ci}

If your code is under version control [@osborne2014ten; @wilson2014best] and hosted online on a website such as GitHub, GitLab or BitBucket you can automate the running of unit tests.
This process is called continuous integration.
In this setup, whenever you push changes in your code from your local computer to the online repository, anytests that you have defined get run automatically.
You can have these systems set up such that you recieve an email if your tests fail for example.
There are various continuous integration services such as travis-ci.org and GitLab pipelines.
These services are often free on a limited basis or free if your code is open source.

A full description of how to set up these services is beyond the scope of this paper.
However, in the simplest cases, the set up is very easy.
For example if you have R code, organised into a package and hosted openly on GitHub, then setting up travis CI is very easy.
Within your version controlled folder you add a file called `.travis.yml` that simply contains a description of which language the code uses.

```{awk, travis, eval = FALSE}
language:r
```

This file can also be setup with the devtools function `use_travis`.
You then need to sign up to travis-ci.org and point it to the correct GitHub repository.
To trigger the first automatic test you need to make a minor change to your code, commit it and push to GitHub.
More details can be found in @wickham2015r Chapter 14.3.



# Conclusions

It is vital that infectious disease models are coded such that bugs are minimised.
Unit testing is the fundemental process by which we can redue the number of bugs in our code.
Setting up unit testing is simple but writing good tests is a skill that takes practice and thought.
There are many frameworks that make testing more automatic and more informative and these should be used where possible.

# Code availability

Please see the fully reproducible and version controlled code at www.github.com/timcdlucas/unit_test_for_infectious_disease for a complete record of the many bugs created and fixed during the writing of this manuscript.


# References
