---
title: |
  | Responsible modelling: Unit testing for infectious disease epidemiology.
author: Tim CD Lucas, Timothy M Pollington, Emma L Davis and  T DÃ©irdre Hollingsworth
output:
  bookdown::pdf_book:
    includes:
      in_header: preamble.tex
    number_sections: true
---
  
```{r, setup, echo=FALSE, results = 'hide', message = FALSE}
rm(list = ls())
library(knitr)
oldSource <- knit_hooks$get("source")
knit_hooks$set(source = function(x, options) {
  x <- oldSource(x, options)
  x <- ifelse(!is.null(options$ref), paste0("\\label{", options$ref, "}", x), x)
  ifelse(!is.null(options$codecap), paste0("\\captionof{chunk}{", options$codecap,"}", x), x)
})

knitr::opts_chunk$set(cache = FALSE, fig.width = 7, fig.height = 4, 
                      out.extra = '', out.width = "0.6\\textwidth",
                      fig.align='center', fig.pos = "h")

# You might need to install the package rticles for the formatting to work.
# To build run
# rmarkdown::render('unit_testing.Rmd')
```

```{r, libs, echo=FALSE, results = 'hide', message = FALSE}
library(ggplot2)
library(testthat)
library(reshape2)
```


# A more realistic modelling example 


Here we define a simple, but more realistic epidemiological model compared to the main text.
We again demonstrate how to effectively write unit tests for it in _R_ code.
We aim to follow the same structure as in the main text but in this example we do not need to worry about the code being short.


We will implement a stochastic, continuous time, SIR model using code from [EpiEcipes](http://epirecip.es/epicookbook/chapters/sir-stochastic-discretestate-continuoustime/r)  as a starting point.
In order to have continuous time dynamics we use the Gillespie algorithm.
In each iteration of the model, exactly one event happens, either an infection or a recovery. 
The time steps in between events are a continuous, positive number that are drawn from an exponential distribution with a rate equal to the sum of two event rates (i.e. infection and recovery).
\newline
```{r first_code,  ref = "firstcode", codecap = "Base example of the multi-pathogen re-infection model", results='hide'}

# set the seed so that our simulations are
#   reproducible.
set.seed(20200908)

# Define parameters
beta <- 0.001 # Tranmsission parameter
gamma <- 0.4 # Recovery parameter
N <- 1000 # Population size
S0 <- 990 # Starting number of susceptibles.
I0 <- 10 # Starting number of infected.
R0 <- N - S0 - I0 # Starting number of Recovered.
tf <- 2000  # Run until time reaches this value.

# Initialise the simulation.
time <- 0
S <- S0
I <- I0
R <- R0
ta <- numeric(0)
Sa <- numeric(0)
Ia <- numeric(0)
Ra <- numeric(0)

# Run the simulation until time reaches
#   tf.
while (time < tf) {
  
    ta <- c(ta, time)
    Sa <- c(Sa, S)
    Ia <- c(Ia, I)
    Ra <- c(Ra, R)
    
    # Infection rate.
    pf1 <- beta * S * I
    # Transmission rate
    pf2 <- gamma * I
    # Total rate of all events
    pf <- pf1 + pf2
    # Draw waiting time for this event.
    dt <- rexp(1, rate = pf)
    # Increment the current time.
    time <- time + dt
    
    # If we have reach time tf, finish the simulation
    if (time > tf) {
        break
    }
    
    # Select whether the event is a transmission
    #   or recovery event.
    if (runif(1) < (pf1 / pf)) {
        # Transmission event.
        S <- S - 1
        I <- I + 1
    } else {
        # Recovery event
        I <- I - 1
        R <- R + 1
    }
    
    # If the epidemic is extinct, end the simulation
    if (I == 0) {
        break
    }
}

results <- data.frame(time = ta, S = Sa, I = Ia, R = Ra)
```

```{r firstplots, echo = FALSE, fig.cap = 'Infection profile for individual 1, who is initially infected with pathogen $a$ but then reinfected with different pathogens', out.width = "75%"}
sir_out_long <- melt(results, "time")
ggplot(sir_out_long, 
       aes(x = time, y = value, colour = variable, group = variable))+
  geom_line(lwd = 2) +
  xlab("Time") + ylab("Number")

```



# Basic unit testing
## Write small functions {-#compactfuns}

To ensure the unit tests are evaluating the exact code as run in the analysis, code should be structured in functions, which can be used to both run unit tests with and to generate results as part of a larger model codebase.
Make your functions compact with a single clearly-defined task.
We have split the code into a number of functions that do relatively well defined tasks such as initialising the simaulation, handling a single transmission event or a single recovery event as well as a large function (`runSim()`) that combines these functions. (\@ref(compactfunctions)).
\newline
```{r, compactfunctions, ref = "compactfunctions", codecap = "Organising code into small functions", result = 'hide'}

initialiseSim <- function(S0, I0, R0){
  sim <- list()
  sim$time <- 0
  sim$S <- S0
  sim$I <- I0
  sim$R <- R0
  sim$ta <- numeric(0)
  sim$Sa <- numeric(0)
  sim$Ia <- numeric(0)
  sim$Ra <- numeric(0)
  return(sim)
}

addNew <- function(sim){
  sim$ta <- c(sim$ta, sim$time)
  sim$Sa <- c(sim$Sa, sim$S)
  sim$Ia <- c(sim$Ia, sim$I)
  sim$Ra <- c(sim$Ra, sim$R)
  return(sim)
}

infection <- function(sim){
  sim$S <- sim$S - 1
  sim$I <- sim$I + 1
  return(sim)
}

recovery <- function(sim){
  sim$I <- sim$I - 1
  sim$R <- sim$R + 1
  return(sim)
}

nextEvent <- function(sim, pf1, pf){
  # Select whether the event is a transmission
  #   or recovery event.
  if (runif(1) < (pf1 / pf)) {
    # Transmission event.
    sim <- infection(sim)
  } else {
    # Recovery event
    sim <- recovery(sim)
  }
  return(sim)
}

calcRates <- function(sim, beta, gamma){
  rates <- list()
  # Infection rate.
  rates$pf1 <- beta * sim$S * sim$I
  # Transmission rate
  rates$pf2 <- gamma * sim$I
  # Total rate of all events
  rates$pf <- rates$pf1 + rates$pf2
  # Draw waiting time for this event.
  rates$dt <- rexp(1, rate = rates$pf)
  return(rates)
}



runSim <- function(sim, beta, gamma, tf){
  # Run the simulation until time reaches
  #   tf.
  while (sim$time < tf) {
      
      sim <- addNew(sim)
      
      rates <- calcRates(sim, beta, gamma)

      # Increment the current time.
      sim$time <- sim$time + rates$dt
      
      # If we have reach time tf, finish the simulation
      if (sim$time > tf) {
          break
      }
      
      sim <- nextEvent(sim, rates$pf1, rates$pf)      
      # If the epidemic is extinct, end the simulation
      if (sim$I == 0) {
          break
      }
  }
  results <- data.frame(time = sim$ta, S = sim$Sa, I = sim$Ia, R = sim$Ra)
  return(results)
}
  
```


As an informal first check we can plot a single simulation and check that it gives similar results to the earlier plot and behaves broadly how we would expect an SIR model to behave.
For example, we expect the number of infectious individuals to increase and then decrease again while we expect the susceptible and recovered classes to monotonically decrease and increase respectively.
\newline
```{r, checksim, ref = "checksim", codecap = "Run a full simulation with the code organised in functions.", result = 'hide'}
sim <- initialiseSim(S0, I0, R0)

simulation <- runSim(sim, beta, gamma, tf)

sir_out_long <- melt(simulation, "time")
ggplot(sir_out_long, 
       aes(x = time, y = value, colour = variable, group = variable))+
  geom_line(lwd = 2) +
  xlab("Time") + ylab("Number")

```

## Test simple cases first {-#easycases}

As there are quite a few functions already in our code we will not write unit tests for all of them here.
However, in (\@ref(testsimplefirst)) we will write some tests for simple aspects of the code.
For example we can confirm that the `infection()` function works as expected.
We expect this function to increase the number of infectious individuals by one, decrease the number of susceptible individuals by one and leave the rest of the object unchanged.
If we start with 1 susceptible and 0 infectious or recovered individuals, this function should result in 0 susceptible individuals, 1 infectious individual and 0 recovered individuals.
We can see that the way we have written the code has separated the process of finding out what event happens from the book keeping of making the change once an event has been selected. 
With 0 infectious individuals we would not expect to ever has an infection event, but separating the functionality of the code allows us to test that the book keeping works without worrying about other aspects.
\newline
```{r, test_simple_first, ref = "testsimplefirst", codecap = "Using simple parameter sets we can work out beforehand what results to expect", results = 'hide'}

sim1 <- initialiseSim(1, 0, 0)
sim2 <- infection(sim1)

# Check that the classes are change by the right amount.
expect_equal(sim2$S, 0)
expect_equal(sim2$I, 1)
expect_equal(sim2$R, 0)

# Check that some other elements of the object are unchanged.
expect_equal(sim2$time, sim1$time)

```





## Test all arguments {-#testargs}

`calcRates()` has three arguments to check.
If we consider a baseline population with one susceptible, one infectious and zero recovered individuals, with both beta and gamma set to one we can easily work out the expected outputs.
In \@ref(testallargs) we will focus our tests on the two rates: the transmission rate `pf1` and the recovery rate `pf2`.
Given that the SIR transmission rate is $\beta S I$ and given that $\beta$, $S$ and $I$ are all one, `pf1` should be one.
Similarly, given a recovery rate of $\gamma I$ and with $\gamma$ set to one, `pf2` should also be one.
\newline
```{r, test_all_args, ref = "testallargs", codecap = "Baseline before testing all function arguments in turn", results = 'hide'}

sim1 <- initialiseSim(S0 = 1, I0 = 1, R0 = 0)
r1 <- calcRates(sim = sim1, beta = 1, gamma = 1)

expect_equal(r1$pf1, 1)
expect_equal(r1$pf2, 1)
```
From there we can alter each of the three parameters in turn and check the behaviour is as expected (\@ref(testallargs2)).
First we will set $\beta$ to 2 and so we expect `pf1` to be 2 while `pf2` should not change.
First we will set $\gamma$ to 2 and so we expect `pf2` to be 2 while `pf1` should not change.
Finally, we will initialise a new simulation with  `I0 = 2` and we will then expect `pf1` and `pf2` to be equal to 2.

```{r, test_all_args2, ref = "testallargs2", codecap = "Test all function arguments in turn", results = 'hide'}

sim1 <- initialiseSim(S0 = 1, I0 = 1, R0 = 0)

# Set beta = 2
r2 <- calcRates(sim = sim1, beta = 2, gamma = 1)

expect_equal(r2$pf1, 2)
expect_equal(r2$pf2, 1)


# Set gamma = 2
r3 <- calcRates(sim = sim1, beta = 1, gamma = 2)

expect_equal(r3$pf1, 1)
expect_equal(r3$pf2, 2)


# Set I0 = 2
sim2 <- initialiseSim(S0 = 1, I0 = 2, R0 = 0)

r4 <- calcRates(sim = sim2, beta = 1, gamma = 1)

expect_equal(r4$pf1, 2)
expect_equal(r4$pf2, 2)
```



## Does the function logic meet your expectations? {-#complexcases}

We can also cover cases that expose deviations from the logical structure of the system.
After initialising our population, we expect all the rows other than the first to contain `NA`.
We also expect each of the pathogens $a$, $b$ and $c$ to occur at least once on the first row if `pathogens` $= 3$ and `N` $\geq 3$.
Finally, `updatePop()` performs a single simulation time step, so we expect only one additional row to be populated.
Instead of testing by their numerical values, we verify logical statements of the results within our macro understanding of the model system (\@ref(testcomplex)).
\newline
```{r, test_complex, ref = "testcomplex", codecap = "Test more complex cases using your understanding of the system", results = 'hide'}

```



The function `addNew()` changes the object ready for a new iteration in the simulation.
Therefore the length of a number of elements should increase by one.
\newline
```{r, test_simple_second, ref = "testsimplesecond", codecap = "It is use", results = 'hide'}

sim1 <- initialiseSim(S0, I0, R0)
sim2 <- infection(sim)

# Check that the classes are change by the right amount.
expect_equal(sim2$S, sim1$S - 1 )
expect_equal(sim2$I, sim1$I + 1 )

# Check that some other elements of the object are unchanged.
expect_equal(sim2$R, sim1$R)
expect_equal(sim2$time, sim1$time)

```


## Combine simple functions and test them at a higher-level{-#combine}

In the end an entire model only runs when its functions work together seamlessly. 
So we next check their connections; achieved through nesting functions together, or defining them at a higher level and checking the macro aspects of the model.
We have already defined a function `runSim()` that runs a few different smaller functions to perform one full simulation.
We would expect the output from `runSim()` to be a dataframe with four columns and no `NA`s and we would no values of the time column to be greater than `tf`. 
We would also expect all of the values in the S, I and R columns to be integers greater than or equal to zero.
There are all higher level properties that should be true regardless of the exact realisation of the simulation.
Furthermore, checking the maximum value in the time column is, to an extent, an emergent property that could not be testing at a lower level.
\newline
```{r, combine_simple_func, ref = "combinesimplefunc", codecap = "Combine simple functions through nesting to check higher-level functionality", result = 'hide'}

set.seed(13131)
sim <- initialiseSim(S0 = 900, I0 = 100, R0 = 0)

tf <- 20
simulation <- runSim(sim = sim, beta = 0.1, gamma = 0.04, tf = tf)

# Check the shape of the output
expect_true(inherits(simulation, 'data.frame'))
expect_equal(ncol(simulation), 4)

# Test for NAs
expect_true(all(!is.na(simulation)))

# Check that all but 1 time point is less than tf.
expect_true(all(simulation$time < tf))

# Check that S, I and R are positive integers.
#   R is not very strict about integers vs doubles.
#   So we instead just check that all the values are
#   very close to whole numbers.
expect_true(all(simulation[, -1] >= 0))
expect_true(all((simulation[, -1] - round(simulation[, -1])) < 1e-10))

```

# Stochastic code

Stochastic simulations are a common feature in infectious disease models.
Stochastic events are difficult to test effectively because, by definition, we do not know beforehand what the result will be.
We can check very broad-scale properties, like \@ref(combinesimplefunc), where we check the range of pathogen values.
However, code could still pass and be wrong (for example the base example (\@ref(firstcode)) would still pass that test).
There are however a number of approaches that can help.

## Split stochastic and deterministic parts {-#splitstochastic}

Isolate the stochastic parts of your code.
For example, `updatePop()` performs stochastic and deterministic operations in one line (\@ref(compactfunctions)).
Firstly, `updatePop()` stochastically samples who gets infected by whom at iteration `t`.
Then it takes those infection events and assigns the new infectious status for each individual.
We demonstrate in \@ref(splitdeterstoch) how this could be split.
We accept this is a fairly exaggerated example and splitting a single line of code into two functions is rare!
  The more common scenario is splitting a multi-line function into smaller functions which also brings benefits of code organisation so it does not feel like extra effort. 
\newline
```{r, split_deter_stoch, ref = "splitdeterstoch", codecap = "Isolation of the determistic and stochastic elements"}

```

Now, half of `updatePop()` is deterministic so can be checked as previously discussed.
We still have `chooseInfector()` that is irreducibly stochastic.
We now examine some techniques for directly testing the stochastic parts of a model.

## Pick a smart parameter for a deterministic result{-#deterministicparams}

In the same way that we used simple parameters values in \@ref(testsimplefirst), we can often find simple cases for which our stochastic functions become deterministic.
For example, samples from $X\sim\text{Bernoulli}(p)$ will always be zeroes for $p=0$ or ones for $p=1$.
In the case of a single pathogen (\@ref(teststochdetermin)), the model is no longer stochastic.
So initialisation with one pathogen means the second time step should equal the first.
\newline
```{r, test_stoch_determin, ref = "teststochdetermin", codecap = "A stochastic function can output deterministically if you can find the right parameter set.", results = 'hide'}

```

## Test all possible answers (if few) {-#allpossible}

Working again with a simple parameter set, there are some cases where the code is stochastic, but with a small, finite set of outputs. 
So we can run the function exhaustively and check it returns all of the possible outputs.
For a population of two people, `chooseInfector()` returns a length-2 vector with the possible elements of 1 or 2.
There are four possibilities when drawing who is infected by whom.
Both individuals can be infected by individual 1, giving the vector {1, 1}. 
Both individuals can be infected by individual 2, giving {2, 2}. 
Both individuals can infect themselves, giving {1, 2}. 
Or finally both individuals can infect each other, giving {2, 1}.
In (\@ref(teststochfewvalues)), `chooseInfector(N = 2)` returns a length-2 vector with the indices of the infector for each infectee. 
`paste0()` then turns this length-2 vector into a length-1 string with two characters; we expect this to be one of "11", "22", "12" or "21".
`replicate()` runs the expression 300 times, but in your unit test you should choose a value high enough so that you are confident that all of the distinct outcomes will have occurred at least once.
\newline
```{r, test_stoch_fewvalues, ref = "teststochfewvalues", codecap = "Testing stochastic output when it only covers a few finite values", results = 'hide'}

```

## Use very large samples for the stochastic part {-#largesamples}

While the previous example worked well for a small set of possible outputs, testing can conversely be made easier by using very large numbers.
This typically involves large sample sizes or numbers of stochastic runs.
For example, the clearest test to distinguish between our original, buggy code (\@ref(firstcode)) and our correct code (\@ref(correctcode)) is that in the correct code there is the possibility for an individual to infect more than one individual in a single time step.
In any given run this is never guaranteed, but the larger the population size the more likely it is to occur. 
With a population of one thousand, the probability that no individual infects two others is vanishingly rare (\@ref(teststochlargenum)).
As this test is now stochastic we should set the seed of the random number generator so that the test is reproducible.
Setting the seed with `set.seed` means that each time the code is run, the same pseudo-random numbers will be generated.
\newline
```{r, test_stoch_largenum, ref = "teststochlargenum", codecap = "Testing that the code does allow one individual to infect multiple individuals.", results='hide'}
set.seed(10261985)
```

If we have an event that we know should never happen, we can use a large number of simulations to provide stronger evidence that it does not stochastically occur.
However, it can be difficult to determine how many times is reasonable to run a simulation, especially if time is short.
This strategy works best when we have a specific bug that occurs relatively frequently (perhaps once every ten simulations or so).
If the bug occurs every ten simulations and we have not fixed it we would be confident that it will occur at least once if we run the simulation 500 or 1000 times.
Conversely, if the bug does not occur even once in 500 or 1000 simulations we can be fairly sure we have fixed it.
\newline
\newline
Similarly, a bug might cause an event that should be rare to happen very regularly or even every time the code is run.
In our original buggy code (\@ref(firstcode)) we found that the proportions remained identical for entire simulations.
We would expect this to happen only very rarely.
We can run a large number of short simulations to check that this specific bug is not still occurring by confirming that the proportion of each pathogen is not always the same between the first and last time point.
As long as we find at least one simulation where the proportions of each pathogen are different between the first and last iteration, we know the bug has been fixed.
\newline
```{r, returningpathogen,ref = "returningpathogen", codecap = "Assessing if a bug fix was a likely success with large code runs, when the bug was appearing relatively frequently"}
set.seed(11121955)

```

# Further testing

## Test incorrect inputs {-#testincorrect}

As well as testing that functions work when given the correct inputs, we must also test that they behave sensibly when given wrong ones.
This typically involves the user inputting argument values that do not make sense.
This may be, for example, because the inputted argument values are the wrong class, in the wrong numeric range or have missing data values.
Therefore it is useful to test that functions fail gracefully if they are given incorrect inputs.
This is especially true for external, exported functions, available to a user on a package's front-end.
However, it is not always obvious what constitutes an 'incorrect value' even to the person who wrote the code.
In some cases, inputting incorrect argument values may cause the function to fail quickly. 
In other cases code may run silently giving false results or take a long time to give an error.
Both of these cases can be serious or annoying and difficult to debug afterwards.
\newline
\newline
Often for these cases, the expected behaviour of the function should be to give an error.
There is no correct output for an epidemiological model with -1 pathogens. 
Instead the function should give an informative error message.
Often the simplest solution is to use defensive programming and include argument checks at the beginning of functions.
We then have to write slightly unintuitive tests for an expression where the expected behaviour is an error.
If the expression does not throw an error the test should throw an error (as this is not the expected behaviour).
Conversely, if the expression does throw an error the test should pass and not throw an error. 
We can use the `expect_error()` function for this task.
This function takes an expression as its first argument and reports an error if the given expression does not throw an error as expected.
\newline
\newline
We can first check that the code sensibly handles the user inputting a string instead of an integer for the number of pathogens.
Because this expression throws an error, `expect_error()` does not throw an error and the test passes.
\newline
```{r, wrong1, ref = "wrong1", codecap = "Testing incorrect pathogen inputs", warning = FALSE}
```

Now we contrast what happens if the user inputs a vector of pathogens to the `initialisePop()` function.
Here we are imagining that the users intent wass to run a simulation with three pathogens: 1, 2 and 3.
\newline
```{r, wrong1b, ref = "wrong1b", codecap = "A failing test for incorrect pathogen inputs", eval = FALSE}
```

This test fails because the function does not throw an error.
Instead the code takes the first element of `pathogens` and ignores the rest.
Therefore, a population is created with one pathogen, not three, which is almost certainly not what the user wanted.
Here, the safest fix is to add an explicit argument check at the top of the function as implemented below.
The same test now passes because `initialisePop()` throws an error when a vector is supplied to the `pathogens` argument.
\newline
```{r, wrong1c, ref = "wrong1c", codecap = "New definition, using defensive programming, of the initialisePop() function  and a passing test for incorrect pathogen inputs"}

```

We can similarly check how the code handles a user inputting a vector of numbers to the `t` argument (perhaps thinking it needed a vector of all time points to run).
In \@ref(wrong1c), `initialisePop()` does not throw an error if a vector is supplied to `t`. 
However, `fullSim()` does throw an error if a vector is supplied to `t`.
While it is a good thing that `fullSim()` throws an error, the error message is not very informative.
If the code that runs before the error is thrown (in this case the `initialisePop()` function) takes a long time, it can also be time consuming to work out what threw the error.
It is also a signature of fragile code that the error is coincidental; a small change in the code might stop the error from occurring.
These considerations all point towards defensive programming as a good solution.
We can add an additional argument check to `initialisePop()`.
Importantly, we then want to check that `fullSim()` errors in the correct place (i.e. in `initialisePop()` rather than afterwards).
We can achieve this using the `regexp` argument of `expect_error()` that compares the actual error message to the expected error messages.
The test will only pass if the error message contains the string provided.
\newline
```{r, wrong2, ref = "wrong2", codecap = "Another new definition of the initialisePop() function and a passing test for the fullSim() function."}


```

## Test edge cases and special cases {-#corners}

When writing tests it is easy to focus on standard behaviour.
However, bugs often occur at _edge cases_---when parameters are at their extrema or at special values.
For example, in _R_, selecting two or more columns from a matrix e.g. `my_matrix[,2:3]` returns a matrix while selecting one column e.g. `my_matrix[,2]` returns a vector.
Code that relies on the returned object being a matrix would fail in this edge case.
\newline
\newline
Similarly, special cases can be triggered with parameter sets that do not match the extrema of parameter space. 
This is where understanding of the functional form of the model can help. 
Consider a function `divide(x, y)` that divides `x` by `y`.
We could test this function by noting that `y * divide(x, y)` should return `x`.
If we write code that tests standard values of `x` and `y` such as `2 * divide(3, 2) == 3` we would believe the function works for nearly all values of division, unless we ever try `y = 0`.
\newline
\newline
We checked earlier if the `pathogens` argument of `initialisePop()` worked by verifying that the returned population had the correct number of pathogens.
However, if we set the `pathogens` argument to be greater than the number of individuals in the population we get a population with `N` pathogens.
The function does not therefore pass the test we defined in \@ref(testallargs).
\newline
```{r, edge2, ref = "edge2", codecap = "initialisePop() does not give a population with the correct number of pathogens if N is less than the number of pathogens.", eval = FALSE}
```

For edge cases like this it may be rather subjective what the correct behaviour should be.
It might be appropriate for the function to throw an error or give a warning if the user requests more pathogens than individuals.
Here however, we will decide that this behaviour is acceptable.
The test above was still useful to highlight this unusual case.
As our expected output from the function has changed, we should change our test; we now expect a population with `N` pathogens.
We should however retain the test in \@ref(testallargs) so that we have two tests: one test checks that when `pathogens < N`, the number of unique pathogens in the population is equal to `pathogens`; the other test checks that when `pathogens > N`, the number of unique pathogens is equal to `N`.
\newline
```{r, edge3, ref = "edge3", codecap = "Check that pathogens is equal to N", eval = FALSE}
```


